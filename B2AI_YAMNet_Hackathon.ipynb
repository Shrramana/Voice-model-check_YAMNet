{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import shutil\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import numpy as np\n",
    "import scipy.io.wavfile as wavfile\n",
    "import scipy.signal\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "File collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Load the CSV file into a DataFrame\n",
    "csv_path = \"C:/Users/shrra/Downloads/2024-06-25T13-17_export.csv\"\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "# Select relevant columns\n",
    "selected_columns = ['record_id', 'demographics_session_id']\n",
    "df = df[selected_columns]\n",
    "\n",
    "# Create 'subject_id' column\n",
    "df['subject_id'] = 'sub-' + df['record_id'] + '/ses-' + df['demographics_session_id']\n",
    "\n",
    "def get_file_paths(subject_ids, base_dir):\n",
    "    file_paths = []\n",
    "    for subject_id in subject_ids:\n",
    "        session_dir = os.path.join(base_dir, subject_id, 'audio')\n",
    "        if os.path.exists(session_dir):\n",
    "            for file_name in os.listdir(session_dir):\n",
    "                if file_name.endswith('Respiration-and-cough_rec-Respiration-and-cough-Cough-2.wav'):\n",
    "                    file_paths.append(os.path.join(session_dir, file_name))\n",
    "                    break  # Assuming only one relevant file per session\n",
    "        else:\n",
    "            print(f'Session directory {session_dir} does not exist.')\n",
    "    return file_paths\n",
    "\n",
    "# Get file paths for all subjects\n",
    "base_dir = 'C:/Users/shrra/Downloads/Hackathon/bridge2ai-voice-corpus-2-including-sensitive-recordings1/bridge2ai-voice-corpus-2-including-sensitive-recordings1/bids_with_sensitive_recordings'\n",
    "file_paths = get_file_paths(df['subject_id'], base_dir)\n",
    "\n",
    "def copy_files(file_paths, dest_dir):\n",
    "    for file_path in file_paths:\n",
    "        if os.path.exists(file_path):\n",
    "            try:\n",
    "                shutil.copy(file_path, dest_dir)\n",
    "                print(f'Copied {file_path} to {dest_dir}')\n",
    "            except PermissionError:\n",
    "                print(f'Permission denied for file {file_path}')\n",
    "            except shutil.SameFileError:\n",
    "                print(f'File {file_path} is already present in the destination directory')\n",
    "            except Exception as e:\n",
    "                print(f'Error copying file {file_path}: {e}')\n",
    "        else:\n",
    "            print(f'File {file_path} does not exist.')\n",
    "\n",
    "# Define destination directory\n",
    "dest_dir = 'C:/Users/shrra/Downloads/Hackathon/cough'\n",
    "\n",
    "# Ensure the destination directory exists or create it\n",
    "if not os.path.exists(dest_dir):\n",
    "    os.makedirs(dest_dir)\n",
    "\n",
    "# Copy files\n",
    "copy_files(file_paths, dest_dir)\n",
    "\n",
    "print(\"File copying complete.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fila name prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Define the base directory and folder names\n",
    "base_dir = 'C:/Users/shrra/Downloads/Hackathon'\n",
    "folders = {\n",
    "    'cough': 'Cough',\n",
    "    # 'yell': 'Yell',\n",
    "    'breathing': 'Breathing',\n",
    "    'speech': 'Speech'\n",
    "}\n",
    "\n",
    "# Prepare a list to collect file names and labels\n",
    "file_data = []\n",
    "\n",
    "# Iterate over each folder and collect file names and labels\n",
    "for folder_name, label in folders.items():\n",
    "    folder_path = os.path.join(base_dir, folder_name)\n",
    "    if os.path.exists(folder_path):\n",
    "        for file_name in os.listdir(folder_path):\n",
    "            # Full path of the file\n",
    "            file_data.append({'file_name': os.path.join(folder_name, file_name), 'correct_label': label})\n",
    "\n",
    "# Create a DataFrame from the collected data\n",
    "df = pd.DataFrame(file_data)\n",
    "\n",
    "# Define the output CSV file path\n",
    "output_csv_path = 'C:/Users/shrra/Downloads/Hackathon/Inputdata_labels.csv'\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "df.to_csv(output_csv_path, index=False)\n",
    "\n",
    "print(f\"CSV file saved to {output_csv_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yamnet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Top 1 label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import scipy.io.wavfile as wavfile\n",
    "import scipy.signal\n",
    "import csv\n",
    "\n",
    "# Load YAMNet model\n",
    "def load_model():\n",
    "    model_url = 'https://tfhub.dev/google/yamnet/1'\n",
    "    model = hub.load(model_url)\n",
    "    return model\n",
    "\n",
    "# Load class labels from the model\n",
    "def load_class_labels(model):\n",
    "    class_map_path = model.class_map_path().numpy()\n",
    "    class_names = []\n",
    "    with tf.io.gfile.GFile(class_map_path) as csvfile:\n",
    "        reader = csv.DictReader(csvfile)\n",
    "        for row in reader:\n",
    "            class_names.append(row['display_name'])\n",
    "    return class_names\n",
    "\n",
    "# Ensure audio is at the desired sample rate\n",
    "def ensure_sample_rate(original_sample_rate, waveform, desired_sample_rate=16000):\n",
    "    if original_sample_rate != desired_sample_rate:\n",
    "        desired_length = int(round(float(len(waveform)) / original_sample_rate * desired_sample_rate))\n",
    "        waveform = scipy.signal.resample(waveform, desired_length)\n",
    "    return desired_sample_rate, waveform\n",
    "\n",
    "# Normalize waveform to [-1.0, 1.0]\n",
    "def normalize_waveform(waveform):\n",
    "    return waveform / np.max(np.abs(waveform))\n",
    "\n",
    "# Process audio file and make predictions\n",
    "def process_file(file_path, model, class_names):\n",
    "    sample_rate, wav_data = wavfile.read(file_path)\n",
    "    sample_rate, wav_data = ensure_sample_rate(sample_rate, wav_data)\n",
    "    wav_data = normalize_waveform(wav_data)\n",
    "    \n",
    "    # Make predictions\n",
    "    waveform = tf.convert_to_tensor(wav_data, dtype=tf.float32)\n",
    "    scores, embeddings, spectrogram = model(waveform)\n",
    "    \n",
    "    # Get predictions\n",
    "    scores_np = scores.numpy()\n",
    "    mean_scores = np.mean(scores_np, axis=0)\n",
    "    top_class_index = np.argmax(mean_scores)\n",
    "    top_label = class_names[top_class_index]\n",
    "    \n",
    "    return top_label\n",
    "\n",
    "# Main function to read CSV and process each file\n",
    "def main(csv_path, base_dir):\n",
    "    df = pd.read_csv(csv_path)\n",
    "    file_names = df['file_name'].tolist()\n",
    "    \n",
    "    model = load_model()\n",
    "    class_names = load_class_labels(model)\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for file_name in file_names:\n",
    "        file_path = os.path.join(base_dir, file_name)\n",
    "        \n",
    "        if not os.path.isfile(file_path):\n",
    "            print(f\"File not found: {file_path}\")\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            top_label = process_file(file_path, model, class_names)\n",
    "            results.append({\n",
    "                'file_name': file_name,\n",
    "                'top_label': top_label\n",
    "            })\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {file_name}: {e}\")\n",
    "    \n",
    "    results_df = pd.DataFrame(results)\n",
    "    results_df.to_csv('C:/Users/shrra/Downloads/Hackathon/predictions1.csv', index=False)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    csv_path = 'C:/Users/shrra/Downloads/Hackathon/filepath.csv'\n",
    "    base_dir = 'C:/Users/shrra/Downloads/Hackathon'\n",
    "    main(csv_path, base_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All labels but finalsie to only 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load YAMNet model\n",
    "def load_model():\n",
    "    model_url = 'https://tfhub.dev/google/yamnet/1'\n",
    "    model = hub.load(model_url)\n",
    "    return model\n",
    "\n",
    "# Load class labels from the model\n",
    "def load_class_labels(model):\n",
    "    class_map_path = model.class_map_path().numpy()\n",
    "    class_names = []\n",
    "    with tf.io.gfile.GFile(class_map_path) as csvfile:\n",
    "        reader = csv.DictReader(csvfile)\n",
    "        for row in reader:\n",
    "            class_names.append(row['display_name'])\n",
    "    return class_names\n",
    "\n",
    "# Ensure audio is at the desired sample rate\n",
    "def ensure_sample_rate(original_sample_rate, waveform, desired_sample_rate=16000):\n",
    "    if original_sample_rate != desired_sample_rate:\n",
    "        desired_length = int(round(float(len(waveform)) / original_sample_rate * desired_sample_rate))\n",
    "        waveform = scipy.signal.resample(waveform, desired_length)\n",
    "    return desired_sample_rate, waveform\n",
    "\n",
    "# Normalize waveform to [-1.0, 1.0]\n",
    "def normalize_waveform(waveform):\n",
    "    return waveform / np.max(np.abs(waveform))\n",
    "\n",
    "# Process audio file and make predictions\n",
    "def process_file(file_path, model, class_names, target_labels):\n",
    "    sample_rate, wav_data = wavfile.read(file_path)\n",
    "    sample_rate, wav_data = ensure_sample_rate(sample_rate, wav_data)\n",
    "    wav_data = normalize_waveform(wav_data)\n",
    "    \n",
    "    # Make predictions\n",
    "    waveform = tf.convert_to_tensor(wav_data, dtype=tf.float32)\n",
    "    scores, embeddings, spectrogram = model(waveform)\n",
    "    \n",
    "    # Get predictions\n",
    "    scores_np = scores.numpy()\n",
    "    mean_scores = np.mean(scores_np, axis=0)\n",
    "    \n",
    "    # Debugging: Print top scores and labels\n",
    "    top_class_indices = np.argsort(mean_scores)[::-1][:5]\n",
    "    top_labels = [class_names[i] for i in top_class_indices]\n",
    "    top_scores = [mean_scores[i] for i in top_class_indices]\n",
    "    \n",
    "    print(f\"File: {file_path}\")\n",
    "    print(\"Top labels and scores:\")\n",
    "    for label, score in zip(top_labels, top_scores):\n",
    "        print(f\"  {label}: {score:.2f}\")\n",
    "    \n",
    "    # Get the top class index\n",
    "    top_class_index = np.argmax(mean_scores)\n",
    "    top_label = class_names[top_class_index]\n",
    "    \n",
    "    # Return top label if it is in the target labels\n",
    "    if top_label in target_labels:\n",
    "        return top_label\n",
    "    else:\n",
    "        return 'Unknown'\n",
    "\n",
    "# Main function to read CSV and process each file\n",
    "def main(csv_path, base_dir):\n",
    "    df = pd.read_csv(csv_path)\n",
    "    file_names = df['file_name'].tolist()\n",
    "    \n",
    "    model = load_model()\n",
    "    class_names = load_class_labels(model)\n",
    "    \n",
    "    # Define the target labels of interest\n",
    "    target_labels = {'Breathing', 'Speech', 'Cough', 'Yell'}\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for file_name in file_names:\n",
    "        file_path = os.path.join(base_dir, file_name)\n",
    "        \n",
    "        if not os.path.isfile(file_path):\n",
    "            print(f\"File not found: {file_path}\")\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            top_label = process_file(file_path, model, class_names, target_labels)\n",
    "            results.append({\n",
    "                'file_name': file_name,\n",
    "                'top_label': top_label\n",
    "            })\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {file_name}: {e}\")\n",
    "    \n",
    "    results_df = pd.DataFrame(results)\n",
    "    results_df.to_csv('C:/Users/shrra/Downloads/Hackathon/predictions.csv', index=False)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    csv_path = 'C:/Users/shrra/Downloads/Hackathon/filepath.csv'\n",
    "    base_dir = 'C:/Users/shrra/Downloads/Hackathon'\n",
    "    main(csv_path, base_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "# Function to calculate accuracy, confusion matrix, and generate pie chart\n",
    "def analyze_predictions(predictions_file, labels_file):\n",
    "    # Load the predictions and labels CSV files\n",
    "    predictions_df = pd.read_csv(predictions_file)\n",
    "    labels_df = pd.read_csv(labels_file)\n",
    "    \n",
    "    # Ensure that both files have the same 'file_name' column\n",
    "    if not predictions_df['file_name'].equals(labels_df['file_name']):\n",
    "        raise ValueError(\"File names in predictions and labels do not match.\")\n",
    "    \n",
    "    # Merge the two dataframes on 'file_name'\n",
    "    merged_df = pd.merge(predictions_df, labels_df, on='file_name')\n",
    "    \n",
    "    # Compare predictions to actual labels\n",
    "    correct_predictions = merged_df['top_label'] == merged_df['correct_label']\n",
    "    accuracy = correct_predictions.mean() * 100  # Convert to percentage\n",
    "    \n",
    "    print(f'Accuracy: {accuracy:.2f}%')\n",
    "    \n",
    "    # Get unique labels\n",
    "    all_labels = pd.concat([merged_df['top_label'], merged_df['correct_label']]).unique()\n",
    "    \n",
    "    # Generate confusion matrix\n",
    "    conf_matrix = confusion_matrix(\n",
    "        merged_df['correct_label'], \n",
    "        merged_df['top_label'], \n",
    "        labels=all_labels\n",
    "    )\n",
    "    \n",
    "    # Plot confusion matrix\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=all_labels, yticklabels=all_labels)\n",
    "    plt.xlabel('Predicted Labels')\n",
    "    plt.ylabel('True Labels')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.show()\n",
    "    \n",
    "    # Plot pie chart for predictions\n",
    "    prediction_counts = merged_df['top_label'].value_counts()\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.pie(prediction_counts, labels=prediction_counts.index, autopct='%1.1f%%', startangle=140)\n",
    "    plt.title('Distribution of Predictions')\n",
    "    plt.show()\n",
    "\n",
    "# Paths to the predictions and labels files\n",
    "predictions_file = 'C:/Users/shrra/Downloads/Hackathon/predictions1.csv'\n",
    "labels_file = 'C:/Users/shrra/Downloads/Hackathon/Inputdata_labels.csv'\n",
    "\n",
    "# Calculate accuracy, confusion matrix, and pie chart\n",
    "analyze_predictions(predictions_file, labels_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Top label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking for all labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # Load YAMNet model\n",
    "# def load_model():\n",
    "#     model_url = 'https://tfhub.dev/google/yamnet/1'\n",
    "#     model = hub.load(model_url)\n",
    "#     return model\n",
    "\n",
    "# # Load class labels from the model\n",
    "# def load_class_labels(model):\n",
    "#     class_map_path = model.class_map_path().numpy()\n",
    "#     class_names = []\n",
    "#     with tf.io.gfile.GFile(class_map_path) as csvfile:\n",
    "#         reader = csv.DictReader(csvfile)\n",
    "#         for row in reader:\n",
    "#             class_names.append(row['display_name'])\n",
    "#     return class_names\n",
    "\n",
    "# # Ensure audio is at the desired sample rate\n",
    "# def ensure_sample_rate(original_sample_rate, waveform, desired_sample_rate=16000):\n",
    "#     if original_sample_rate != desired_sample_rate:\n",
    "#         desired_length = int(round(float(len(waveform)) / original_sample_rate * desired_sample_rate))\n",
    "#         waveform = scipy.signal.resample(waveform, desired_length)\n",
    "#     return desired_sample_rate, waveform\n",
    "\n",
    "# # Normalize waveform to [-1.0, 1.0]\n",
    "# def normalize_waveform(waveform):\n",
    "#     return waveform / np.max(np.abs(waveform))\n",
    "\n",
    "# # Process audio file and make predictions\n",
    "# def process_file(file_path, model, class_names):\n",
    "#     sample_rate, wav_data = wavfile.read(file_path)\n",
    "#     sample_rate, wav_data = ensure_sample_rate(sample_rate, wav_data)\n",
    "#     wav_data = normalize_waveform(wav_data)\n",
    "    \n",
    "#     # Make predictions\n",
    "#     waveform = tf.convert_to_tensor(wav_data, dtype=tf.float32)\n",
    "#     scores, embeddings, spectrogram = model(waveform)\n",
    "    \n",
    "#     # Get predictions\n",
    "#     scores_np = scores.numpy()\n",
    "#     mean_scores = np.mean(scores_np, axis=0)\n",
    "#     top_class_indices = np.argsort(mean_scores)[::-1][:5]\n",
    "#     top_labels = [class_names[i] for i in top_class_indices]\n",
    "    \n",
    "#     return top_labels\n",
    "\n",
    "# # Main function to read CSV and process each file\n",
    "# def main(csv_path, base_dir):\n",
    "#     df = pd.read_csv(csv_path)\n",
    "#     file_names = df['file_name'].tolist()\n",
    "    \n",
    "#     model = load_model()\n",
    "#     class_names = load_class_labels(model)\n",
    "    \n",
    "#     results = []\n",
    "    \n",
    "#     for file_name in file_names:\n",
    "#         file_path = os.path.join(base_dir, file_name)\n",
    "        \n",
    "#         if not os.path.isfile(file_path):\n",
    "#             print(f\"File not found: {file_path}\")\n",
    "#             continue\n",
    "        \n",
    "#         try:\n",
    "#             top_labels = process_file(file_path, model, class_names)\n",
    "#             results.append({\n",
    "#                 'file_name': file_name,\n",
    "#                 'top_labels': ', '.join(top_labels)\n",
    "#             })\n",
    "#         except Exception as e:\n",
    "#             print(f\"Error processing {file_name}: {e}\")\n",
    "    \n",
    "#     results_df = pd.DataFrame(results)\n",
    "#     results_df.to_csv('C:/Users/shrra/Downloads/Hackathon/predictions2.csv', index=False)\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#     csv_path = 'C:/Users/shrra/Downloads/Hackathon/filepath.csv'\n",
    "#     base_dir = 'C:/Users/shrra/Downloads/Hackathon'\n",
    "#     main(csv_path, base_dir)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
